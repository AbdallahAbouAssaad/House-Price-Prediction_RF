# -*- coding: utf-8 -*-
"""HousePricing_DecisionTree

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H_7Go869U0TQNfl2kathmORudQlQObe0
"""

import pandas as pd

# Load your dataset
df = pd.read_csv("house_prices.csv")

# Quick look
print(df.shape)       # rows, columns
print(df.head())      # first rows

X = df.drop("price", axis=1)  # features
Y = df["price"]               # target

numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_features = X.select_dtypes(include=['object']).columns.tolist()

print("Numeric features:", numeric_features)
print("Categorical features:", categorical_features)

from sklearn.impute import SimpleImputer

num_imputer = SimpleImputer(strategy="median")
cat_imputer = SimpleImputer(strategy="most_frequent")

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Categorical pipeline: fill missing values → encode
cat_pipeline = Pipeline(steps=[
    ("imputer", cat_imputer),
    ("encoder", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", num_imputer, numeric_features),
        ("cat", cat_pipeline, categorical_features)
    ]
)

from sklearn.tree import DecisionTreeRegressor

dt_pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("model", DecisionTreeRegressor(
        random_state=1,
        max_depth=5,        # limit depth
        min_samples_leaf=4  # minimum samples per leaf
    ))
])

from sklearn.model_selection import train_test_split

# Split data (80% train, 20% test is standard)
X_train, X_test, y_train, y_test = train_test_split(
    X, Y, test_size=0.2, random_state=1
)

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Fit on training data
dt_pipeline.fit(X_train, y_train)

# Predict
y_pred = dt_pipeline.predict(X_test)

# Evaluate
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

print("Decision Tree MSE:", mse)
print("Decision Tree R²:", r2)
print("Decision Tree MAE:", mae)

from sklearn.ensemble import RandomForestRegressor

# Random Forest pipeline
rf_pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("model", RandomForestRegressor(
        n_estimators=100,    # number of trees
        random_state=1,
        n_jobs=-1            # use all CPU cores
    ))
])
# Fit and predict
rf_pipeline.fit(X_train, y_train)
y_pred_rf = rf_pipeline.predict(X_test)

# Evaluate
print("Random Forest MSE:", mean_squared_error(y_test, y_pred_rf))
print("Random Forest R²:", r2_score(y_test, y_pred_rf))
print("Random Forest MAE:", mean_absolute_error(y_test, y_pred_rf))

print(Y.mean())

import matplotlib.pyplot as plt

# Colors
actual_color = 'black'
dt_color = 'blue'
rf_color = 'green'

plt.figure(figsize=(8,6))

# Plot perfect prediction line
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, label='Perfect Prediction')

# Scatter plots
plt.scatter(y_test, y_test, color=actual_color, alpha=0.3, label='Actual Prices')  # actuals along y=x line
plt.scatter(y_test, y_pred, color=dt_color, alpha=0.6, label='Decision Tree')
plt.scatter(y_test, y_pred_rf, color=rf_color, alpha=0.6, label='Random Forest')

# Labels and title
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Predicted vs Actual Prices: Decision Tree vs Random Forest')
plt.legend()
plt.grid(True)

# Save figure
plt.savefig('actual_dt_rf_pred_vs_actual.png')
plt.close()